{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e37d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 加载测试集 ===\n",
      "测试集加载完成，共 511 条数据\n",
      "\n",
      "=== 准备测试集数据 ===\n",
      "测试数据创建完成，共 511 条有效数据\n",
      "缺失文件: 0 个\n",
      "\n",
      "=== 预处理测试集文本 ===\n",
      "\n",
      "=== 加载训练时保存的模型 ===\n",
      "训练集文本特征形状: (3600, 2000)\n",
      "训练集图像特征形状: (3600, 2048)\n",
      "加权拼接融合: 文本权重0.8, 图像权重0.2\n",
      "融合后维度: 4048\n",
      "融合特征标准化器已拟合，输入维度: 4048\n",
      "PCA已拟合，解释方差比: 0.5488\n",
      "\n",
      "=== 提取测试集文本特征 ===\n",
      "测试集文本特征形状: (511, 2000)\n",
      "\n",
      "=== 提取测试集图像特征 ===\n",
      "加载模型: resnet50\n",
      "特征维度: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "提取特征: 100%|██████████| 16/16 [00:16<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征提取完成:\n",
      "  成功: 511 张图像\n",
      "  失败: 0 张图像\n",
      "测试集图像特征形状: (511, 2048)\n",
      "\n",
      "=== 特征标准化和融合 ===\n",
      "标准化后文本特征形状: (511, 2000)\n",
      "标准化后图像特征形状: (511, 2048)\n",
      "加权拼接融合: 文本权重0.8, 图像权重0.2\n",
      "融合后维度: 4048\n",
      "融合后特征形状: (511, 4048)\n",
      "标准化融合特征形状: (511, 4048)\n",
      "\n",
      "=== 特征降维 ===\n",
      "PCA降维后特征形状: (511, 512)\n",
      "\n",
      "=== 进行预测 ===\n",
      "预测完成，共 511 个预测结果\n",
      "\n",
      "=== 映射预测标签 ===\n",
      "预测标签分布:\n",
      "  negative: 201\n",
      "  neutral: 51\n",
      "  positive: 259\n",
      "\n",
      "=== 创建提交文件 ===\n",
      "提交文件已保存到: ./data/test_predictions.csv\n",
      "\n",
      "前10个预测结果:\n",
      "   guid       tag\n",
      "0     8  positive\n",
      "1  1576  negative\n",
      "2  2320  positive\n",
      "3  4912  negative\n",
      "4  3821  positive\n",
      "5  1306   neutral\n",
      "6  4555  positive\n",
      "7   259  negative\n",
      "8  3216  negative\n",
      "9   881  positive\n",
      "\n",
      "=== 生成详细报告 ===\n",
      "详细预测结果已保存到: ./data/test_detailed_predictions.csv\n",
      "\n",
      "=== 测试集预测完成 ===\n",
      "总测试样本: 511\n",
      "成功处理: 511\n",
      "文件缺失: 0\n",
      "提交文件: ./data/test_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "randomseed = 42\n",
    "\n",
    "# 1. 加载测试集guid\n",
    "print(\"=== 加载测试集 ===\")\n",
    "\n",
    "def load_test_guid(test_file_path='./data/test_without_label.txt'):\n",
    "    \"\"\"加载测试集的guid\"\"\"\n",
    "    test_df = pd.read_csv(test_file_path, dtype={'guid': str})\n",
    "    print(f\"测试集加载完成，共 {len(test_df)} 条数据\")\n",
    "    return test_df\n",
    "\n",
    "# 加载测试集guid\n",
    "test_df = load_test_guid()\n",
    "\n",
    "# 2. 文本预处理类\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, language='english'):\n",
    "        self.language = language\n",
    "        self.stop_words = set(stopwords.words(language))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        self.url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "        self.mention_pattern = r'@\\w+'\n",
    "        self.hashtag_pattern = r'#\\w+'\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        text = text.lower()\n",
    "        text = re.sub(self.url_pattern, '', text)\n",
    "        text = re.sub(self.mention_pattern, '', text)\n",
    "        text = re.sub(self.hashtag_pattern, lambda x: x.group(0)[1:], text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = re.sub(r'[^\\w\\s#@]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        tokens = word_tokenize(text)\n",
    "        return tokens\n",
    "    \n",
    "    def remove_stopwords(self, tokens):\n",
    "        filtered_tokens = [word for word in tokens if word not in self.stop_words]\n",
    "        return filtered_tokens\n",
    "    \n",
    "    def preprocess_pipeline(self, text):\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        tokens = self.tokenize(cleaned_text)\n",
    "        tokens = self.remove_stopwords(tokens)\n",
    "        processed_text = ' '.join(tokens)\n",
    "        return processed_text\n",
    "\n",
    "# 3. 图像处理类\n",
    "class ImageProcessor:\n",
    "    def __init__(self, image_size=(224, 224)):\n",
    "        self.image_size = image_size\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.CenterCrop(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    def load_and_preprocess_image(self, image_path):\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_tensor = self.transform(image)\n",
    "        return image_tensor\n",
    "\n",
    "\n",
    "# 4. 图像特征提取类\n",
    "class ImageFeatureExtractor:\n",
    "    def __init__(self, model_name='resnet50', device=None):\n",
    "        self.model_name = model_name\n",
    "        self.device = torch.device('cpu')\n",
    "        \n",
    "        self.model = self._load_pretrained_model(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.feature_dim = self._get_feature_dimension()\n",
    "        print(f\"加载模型: {model_name}\")\n",
    "        print(f\"特征维度: {self.feature_dim}\")\n",
    "    \n",
    "    def _load_pretrained_model(self, model_name):\n",
    "        model = models.resnet50(pretrained=True)\n",
    "        model = torch.nn.Sequential(*list(model.children())[:-2])\n",
    "        model.add_module('global_avg_pool', torch.nn.AdaptiveAvgPool2d((1, 1)))\n",
    "        model.add_module('flatten', torch.nn.Flatten())\n",
    "        \n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def _get_feature_dimension(self):\n",
    "        dummy_input = torch.randn(1, 3, 224, 224).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            features = self.model(dummy_input)\n",
    "        return features.shape[1]\n",
    "    \n",
    "    def extract_features_batch(self, image_tensors):\n",
    "        if not image_tensors:\n",
    "            return np.array([])\n",
    "        \n",
    "        batch_tensor = torch.stack(image_tensors).to(self.device)\n",
    "        features_list = []\n",
    "        batch_size = 32\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(batch_tensor), batch_size):\n",
    "                batch = batch_tensor[i:i+batch_size]\n",
    "                features = self.model(batch)\n",
    "                features_list.append(features.cpu().numpy())\n",
    "        \n",
    "        all_features = np.vstack(features_list)\n",
    "        return all_features\n",
    "    \n",
    "    def extract_features_from_paths(self, image_paths, processor, batch_size=32):\n",
    "        features = []\n",
    "        success_indices = []\n",
    "        failed_indices = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(image_paths), batch_size), desc=\"提取特征\"):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            batch_tensors = []\n",
    "            batch_success_indices = []\n",
    "            \n",
    "            for j, img_path in enumerate(batch_paths):\n",
    "                img_tensor = processor.load_and_preprocess_image(img_path)\n",
    "                if img_tensor is not None:\n",
    "                    batch_tensors.append(img_tensor)\n",
    "                    batch_success_indices.append(i + j)\n",
    "                else:\n",
    "                    failed_indices.append(i + j)\n",
    "            \n",
    "            if batch_tensors:\n",
    "                batch_features = self.extract_features_batch(batch_tensors)\n",
    "                features.append(batch_features)\n",
    "                success_indices.extend(batch_success_indices)\n",
    "        \n",
    "        if features:\n",
    "            all_features = np.vstack(features)\n",
    "        else:\n",
    "            all_features = np.array([])\n",
    "        \n",
    "        print(f\"特征提取完成:\")\n",
    "        print(f\"  成功: {len(success_indices)} 张图像\")\n",
    "        print(f\"  失败: {len(failed_indices)} 张图像\")\n",
    "        \n",
    "        return all_features, success_indices, failed_indices\n",
    "\n",
    "# 5. 多模态特征融合类\n",
    "class MultimodalFeatureFusion:\n",
    "    def __init__(self):\n",
    "        self.text_weight = 0.8\n",
    "        self.image_weight = 0.2\n",
    "\n",
    "    def weighted_concat(self, text_features, image_features, text_weight=0.5, image_weight=0.5):\n",
    "        print(f\"加权拼接融合: 文本权重{text_weight}, 图像权重{image_weight}\")\n",
    "        \n",
    "        text_norm = text_features / (np.linalg.norm(text_features, axis=1, keepdims=True) + 1e-8)\n",
    "        image_norm = image_features / (np.linalg.norm(image_features, axis=1, keepdims=True) + 1e-8)\n",
    "        \n",
    "        weighted_text = text_norm * text_weight\n",
    "        weighted_image = image_norm * image_weight\n",
    "        \n",
    "        fused_features = np.concatenate([weighted_text, weighted_image], axis=1)\n",
    "        print(f\"融合后维度: {fused_features.shape[1]}\")\n",
    "        \n",
    "        return fused_features\n",
    "    \n",
    "    def fuse(self, text_features, image_features):\n",
    "        return self.weighted_concat(text_features, image_features, self.text_weight, self.image_weight)\n",
    "\n",
    "# 6. 加载测试集数据并处理\n",
    "print(\"\\n=== 准备测试集数据 ===\")\n",
    "\n",
    "def load_test_data(test_df, data_dir='./data'):\n",
    "    \"\"\"加载测试集的图片和文本数据\"\"\"\n",
    "    test_records = []\n",
    "    missing_files = []\n",
    "    \n",
    "    for _, row in test_df.iterrows():\n",
    "        guid = str(row['guid']).split('.')[0]  # 确保移除可能的\".0\"后缀\n",
    "        \n",
    "        img_path = os.path.join(data_dir, f\"{guid}.jpg\")\n",
    "        txt_path = os.path.join(data_dir, f\"{guid}.txt\")\n",
    "            \n",
    "        with open(txt_path, 'r', encoding='latin-1') as f:\n",
    "            text_content = f.read().strip()\n",
    "\n",
    "        \n",
    "        test_records.append({\n",
    "            'guid': guid,  # 存储处理后的guid\n",
    "            'image_path': img_path,\n",
    "            'text': text_content\n",
    "        })\n",
    "    \n",
    "    test_data_df = pd.DataFrame(test_records)\n",
    "    print(f\"测试数据创建完成，共 {len(test_data_df)} 条有效数据\")\n",
    "    print(f\"缺失文件: {len(missing_files)} 个\")\n",
    "    \n",
    "    if missing_files:\n",
    "        print(\"前10个缺失文件:\", missing_files[:10])  # 只显示前10个\n",
    "    \n",
    "    return test_data_df\n",
    "\n",
    "# 加载测试数据\n",
    "test_data_df = load_test_data(test_df)\n",
    "\n",
    "# 7. 预处理测试集文本\n",
    "print(\"\\n=== 预处理测试集文本 ===\")\n",
    "\n",
    "# 初始化文本预处理器\n",
    "preprocessor = TextPreprocessor(language='english')\n",
    "\n",
    "# 预处理文本\n",
    "test_texts_processed = test_data_df['text'].apply(\n",
    "    lambda x: preprocessor.preprocess_pipeline(x)\n",
    ")\n",
    "\n",
    "# 保存预处理后的文本\n",
    "test_data_df['text_processed'] = test_texts_processed\n",
    "\n",
    "# 8. 加载训练时保存的模型\n",
    "print(\"\\n=== 加载训练时保存的模型 ===\")\n",
    "\n",
    "# 加载文本向量化器\n",
    "with open('./data/splits/text_vectorizer.pkl', 'rb') as f:\n",
    "    vectorizer_data = pickle.load(f)\n",
    "    vectorizer = vectorizer_data['vectorizer']\n",
    "\n",
    "# 加载训练时保存的标准化器\n",
    "fusion_dir = './data/splits/fusion'\n",
    "\n",
    "# 加载训练集的原始文本和图像特征\n",
    "train_text_features_path = os.path.join('./data/splits', 'train_features.npz')\n",
    "train_text_features = sparse.load_npz(train_text_features_path).toarray()\n",
    "print(f\"训练集文本特征形状: {train_text_features.shape}\")\n",
    "    \n",
    "train_image_features_path = os.path.join('./data/splits', 'train_image_features.npy')\n",
    "train_image_features = np.load(train_image_features_path)\n",
    "print(f\"训练集图像特征形状: {train_image_features.shape}\")\n",
    "\n",
    "# 创建标准化器\n",
    "text_scaler = StandardScaler()\n",
    "image_scaler = StandardScaler()\n",
    "    \n",
    "# 拟合标准化器\n",
    "text_scaler.fit(train_text_features)\n",
    "image_scaler.fit(train_image_features)\n",
    "\n",
    "# 创建特征融合器\n",
    "fusion = MultimodalFeatureFusion()\n",
    "    \n",
    "# 标准化训练集特征\n",
    "train_text_norm = text_scaler.transform(train_text_features)\n",
    "train_image_norm = image_scaler.transform(train_image_features)\n",
    "        \n",
    "# 融合特征\n",
    "train_fused = fusion.fuse(train_text_norm, train_image_norm)\n",
    "        \n",
    "# 创建融合特征标准化器\n",
    "fused_scaler = StandardScaler()\n",
    "fused_scaler.fit(train_fused)\n",
    "print(f\"融合特征标准化器已拟合，输入维度: {fused_scaler.n_features_in_}\")\n",
    "        \n",
    "# PCA降维\n",
    "from sklearn.decomposition import PCA\n",
    "if train_fused.shape[1] > 512:\n",
    "    pca = PCA(n_components=512, random_state=42)\n",
    "    train_fused_reduced = pca.fit_transform(train_fused)\n",
    "    print(f\"PCA已拟合，解释方差比: {np.sum(pca.explained_variance_ratio_):.4f}\")\n",
    "\n",
    "# 加载分类模型\n",
    "classifier = joblib.load('./data/models/baseline_logistic_quick.pkl')\n",
    "\n",
    "# 9. 提取测试集文本特征\n",
    "print(\"\\n=== 提取测试集文本特征 ===\")\n",
    "\n",
    "test_texts = test_data_df['text_processed'].fillna('').tolist()\n",
    "test_text_features = vectorizer.transform(test_texts).toarray()\n",
    "print(f\"测试集文本特征形状: {test_text_features.shape}\")\n",
    "\n",
    "# 10. 提取测试集图像特征\n",
    "print(\"\\n=== 提取测试集图像特征 ===\")\n",
    "\n",
    "# 初始化图像处理器和特征提取器\n",
    "image_processor = ImageProcessor(image_size=(224, 224))\n",
    "image_extractor = ImageFeatureExtractor(model_name='resnet50')\n",
    "\n",
    "# 提取图像特征\n",
    "test_image_paths = test_data_df['image_path'].tolist()\n",
    "test_image_features, test_success_idx, test_failed_idx = image_extractor.extract_features_from_paths(\n",
    "    test_image_paths, image_processor, batch_size=32\n",
    ")\n",
    "\n",
    "print(f\"测试集图像特征形状: {test_image_features.shape}\")\n",
    "\n",
    "# 11. 特征标准化和融合\n",
    "print(\"\\n=== 特征标准化和融合 ===\")\n",
    "\n",
    "# 标准化文本特征\n",
    "test_text_norm = text_scaler.transform(test_text_features)\n",
    "\n",
    "# 标准化图像特征\n",
    "test_image_norm = image_scaler.transform(test_image_features)\n",
    "\n",
    "print(f\"标准化后文本特征形状: {test_text_norm.shape}\")\n",
    "print(f\"标准化后图像特征形状: {test_image_norm.shape}\")\n",
    "\n",
    "# 初始化特征融合器\n",
    "fusion = MultimodalFeatureFusion()\n",
    "\n",
    "# 融合特征\n",
    "test_fused = fusion.fuse(test_text_norm, test_image_norm)\n",
    "print(f\"融合后特征形状: {test_fused.shape}\")\n",
    "\n",
    "# 标准化融合特征\n",
    "test_fused_norm = fused_scaler.transform(test_fused)\n",
    "print(f\"标准化融合特征形状: {test_fused_norm.shape}\")\n",
    "\n",
    "# 12. 特征降维（如果维度大于512）\n",
    "print(\"\\n=== 特征降维 ===\")\n",
    "\n",
    "if test_fused_norm.shape[1] > 512:\n",
    "    test_fused_reduced = pca.transform(test_fused_norm)\n",
    "    test_fused_norm = test_fused_reduced\n",
    "    print(f\"PCA降维后特征形状: {test_fused_norm.shape}\")\n",
    "\n",
    "# 13. 进行预测\n",
    "print(\"\\n=== 进行预测 ===\")\n",
    "\n",
    "# 使用分类模型进行预测\n",
    "test_predictions = classifier.predict(test_fused_norm)\n",
    "print(f\"预测完成，共 {len(test_predictions)} 个预测结果\")\n",
    "\n",
    "# 14. 将预测结果映射回原始标签\n",
    "print(\"\\n=== 映射预测标签 ===\")\n",
    "\n",
    "# 加载训练时使用的标签编码器\n",
    "train_df = pd.read_csv('./data/splits/train_dataset.csv')\n",
    "all_labels = train_df['tag'].unique()\n",
    "\n",
    "# 创建标签编码器\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "# 将数值预测转换为标签\n",
    "test_pred_labels = label_encoder.inverse_transform(test_predictions)\n",
    "\n",
    "# 统计预测分布\n",
    "unique, counts = np.unique(test_pred_labels, return_counts=True)\n",
    "print(\"预测标签分布:\")\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"  {label}: {count}\")\n",
    "\n",
    "# 15. 创建提交文件\n",
    "print(\"\\n=== 创建提交文件 ===\")\n",
    "\n",
    "# 创建包含所有guid的预测结果\n",
    "submission_df = pd.DataFrame({\n",
    "    'guid': test_df['guid'].apply(lambda x: str(x).split('.')[0])  # 移除\".0\"后缀\n",
    "})\n",
    "\n",
    "# 将预测结果添加到DataFrame中\n",
    "# 创建guid到预测标签的映射\n",
    "prediction_dict = dict(zip(test_data_df['guid'], test_pred_labels))\n",
    "\n",
    "# 为每个guid添加预测标签，如果文件缺失则标记为neutral\n",
    "def get_prediction(guid):\n",
    "    \"\"\"根据guid获取预测标签\"\"\"\n",
    "    guid_str = str(guid).split('.')[0]  # 移除\".0\"后缀\n",
    "    if guid_str in prediction_dict:\n",
    "        return prediction_dict[guid_str]\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "submission_df['tag'] = submission_df['guid'].apply(get_prediction)\n",
    "\n",
    "# 保存提交文件\n",
    "submission_path = './data/test_predictions.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "print(f\"提交文件已保存到: {submission_path}\")\n",
    "\n",
    "print(\"\\n前10个预测结果:\")\n",
    "print(submission_df.head(10))\n",
    "\n",
    "# 16. 生成详细报告\n",
    "print(\"\\n=== 生成详细报告 ===\")\n",
    "\n",
    "# 保存预测结果\n",
    "detailed_df = test_data_df.copy()\n",
    "detailed_df['predicted_tag'] = test_pred_labels\n",
    "detailed_df = detailed_df[['guid', 'text', 'image_path', 'predicted_tag']]\n",
    "\n",
    "detailed_path = './data/test_detailed_predictions.csv'\n",
    "detailed_df.to_csv(detailed_path, index=False)\n",
    "print(f\"详细预测结果已保存到: {detailed_path}\")\n",
    "\n",
    "print(\"\\n=== 测试集预测完成 ===\")\n",
    "print(f\"总测试样本: {len(test_df)}\")\n",
    "print(f\"成功处理: {len(test_data_df)}\")\n",
    "print(f\"文件缺失: {len(test_df) - len(test_data_df)}\")\n",
    "print(f\"提交文件: {submission_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
